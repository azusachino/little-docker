# config/pipelines.yml
- pipeline.id: logstash-filebeat
  config.string: |
    input { 
      beats {
        port => "5044"
      }
    }
    filter {
      grok {
        match => [
          "message", "(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}.\d{3}) \[%{NOTSPACE:thread}\] %{NOTSPACE:level} %{NOTSPACE:class} - %{GREEDYDATA:msg}"
        ]
      }
      # 如果匹配失败，则不进行采集
      if "_grokparsefailure" in [tags] {
        drop {}
      }
      # 指定记录的时间戳字段
      date {
        match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
        target => "@timestamp"
      }
      mutate {
        remove_field => "timestamp"
      }  
    }
    output {           
      elasticsearch {
        hosts => ["http://elasticsearch:9200"]
        index => "%{[fields][model]}-%{+YYYY-MM-dd}"
        template_name => "filebeat"
        template_overwrite => true
        template => "/usr/share/logstash/template/template_filebeat.json"
      }
    }
- pipeline.id: logstash-kafka
  pipeline.batch.size: 3000
  pipeline.batch.delay: 10
  config.string: |
    input { 
      kafka { 
        bootstrap_servers => ["kafka1:9092, kafka2:9092, kafka3:9092"]
        topics => ["TOPIC_KAFKA"]
        group_id => "logstash"
        codec => json
      } 
    }
    filter {
      date {
        match => [ "time", "UNIX_MS"]
        timezone => "Asia/Shanghai"
      }
      ruby { 
        code => "event.set('timestamp', event.get('@timestamp').time.localtime + 8*60*60)" 
      }
      ruby {
        code => "event.set('@timestamp',event.get('timestamp'))"
      }
      mutate {
        add_field => { "message" => "%{timestamp} - %{msg}" }
        remove_field => ["time", "timestamp"]
      }
    }
    output {
      elasticsearch {
        hosts => ["http://elasticsearch:9200"]
        index => "kafka-%{uuid}"
        template_name => "kafka"
        template_overwrite => true
        template => "/usr/share/logstash/template/template_index.json"
      }
    }
