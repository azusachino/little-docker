input {
  # Listens on 514/udp and 514/tcp by default; change that to non-privileged port
  # syslog { port => 51415 }
  # gelf { }
  beats {
      type => "filebeat"
      port => "5044"
  }

  kafka {
      type => "kafka"
      bootstrap_servers => "kafka:9092" # kafka from docker compose container_name
      topics => ["default-topic"]
      codec => json
  }
}

# 通过logstash实现filebeat的功能
# input {
#     # 来源beats
#     beats {
#         # 端口
#         port => "5044"
#     }
#     file {
#        type => "server-log"
#        path => "/logs/*.log"
#        start_position => "beginning"
#        codec=>multiline{
#                 // 正则表达式，所有“20”前缀日志, 如果你的日志是以“[2020-06-15”这类前缀则，可以替换成"^["
#                 pattern => "^20"
#                 // 是否对正则规则取反
#                 negate => true
#                 // previous 表示合并到上一行，next 表示合并到下一行
#                 what => "previous"
#         }
#
#     }
# }

# The following filter is a hack! The "de_dot" filter would be better, but it is not pre-installed with logstash by default.
# filter {
#   ruby {
#     code => "event.to_hash.keys.each { |k| event[ k.gsub('.','_') ] = event.remove(k) if k.include?'.' } "
#   }
# }

filter {

  if [type] == "filebeat" {
    grok {
      match => [
        "message", "(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}.\d{3}) \[%{NOTSPACE:thread}\] %{NOTSPACE:level} %{NOTSPACE:class} - %{GREEDYDATA:msg}",
        # 有些情况下，日志级别后有两个空格
        "message", "(?<timestamp>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}.\d{3}) \[%{NOTSPACE:thread}\] %{NOTSPACE:level}  %{NOTSPACE:class} - %{GREEDYDATA:msg}"
      ]
    }
    # 如果匹配失败，则不进行采集
    if "_grokparsefailure" in [tags] {
      drop {}
    }
  }

  if [type] == "kafka" {
    json {
      source => "message"
    }
  }
}

output {
  if [type] == "filebeat" {
      elasticsearch {
          hosts => ["http://elasticsearch:9200"]
          index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
      }
  }

  if [type] == "kafka" {
      elasticsearch {
          hosts => ["http://elasticsearch:9200"]
          index => "kafka-%{+YYYY.MM.dd}"
      }
  }
}
